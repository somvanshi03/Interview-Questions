# Terraform interview questions 

---

## ðŸ“‹ Table of Contents
- [Basic Terraform Questions](#basic-terraform-questions)
- [Intermediate Terraform Questions](#intermediate-terraform-questions)
- [Advanced Terraform Questions](#advanced-terraform-questions)
- [Terraform Scenario-Based Questions](#terraform-scenario-based-questions)
- [Terraform Commands Cheat Sheet](#terraform-commands-cheat-sheet)

---

## Basic Terraform Questions

### Q1: What is Terraform and why is it used?

**Answer:**
Terraform is an **open-source Infrastructure as Code (IaC) tool** developed by HashiCorp that allows you to define, provision, and manage infrastructure resources across multiple cloud providers and on-premises environments using a **declarative configuration language** called HCL (HashiCorp Configuration Language) .

**Primary purposes**:
- **Automate infrastructure provisioning**: Replace manual processes with code
- **Ensure consistency**: Same configuration produces same infrastructure every time
- **Version control**: Infrastructure definitions can be versioned like application code
- **Multi-cloud support**: Manage resources across AWS, Azure, GCP, and others with a single tool 

### Q2: What is Infrastructure as Code (IaC)?

**Answer:**
**Infrastructure as Code (IaC)** is the practice of managing and provisioning infrastructure through machine-readable definition files, rather than through physical hardware configuration or interactive configuration tools .

**Key benefits**:
- **Automation**: Eliminates manual provisioning steps
- **Repeatability**: Same code produces identical environments
- **Version control**: Track infrastructure changes over time
- **Collaboration**: Teams can review and contribute to infrastructure definitions
- **Consistency**: Reduces configuration drift across environments 

### Q3: What are the key features of Terraform?

**Answer:**
Terraform's essential features include :

| Feature | Description |
|---------|-------------|
| **Infrastructure as Code** | Define infrastructure in human-readable, declarative configuration files |
| **Execution Plans** | Preview changes with `terraform plan` before applying them |
| **Resource Graph** | Visual representation of dependencies between resources |
| **Change Automation** | Apply complex changesets with minimal human intervention |
| **State Management** | Track resource metadata for efficient updates and deletions |
| **Provider Ecosystem** | Plugins for over 1,000+ services and platforms |
| **Modularity** | Reusable components for organized infrastructure code |

### Q4: What are the most useful Terraform commands?

**Answer:**
Essential Terraform commands include :

| Command | Purpose |
|---------|---------|
| `terraform init` | Initializes working directory, downloads provider plugins, and sets up backend |
| `terraform plan` | Creates an execution plan showing what changes will be made |
| `terraform apply` | Applies the changes to reach desired state |
| `terraform destroy` | Destroys all resources managed by the configuration |
| `terraform validate` | Validates configuration files for syntax errors |
| `terraform fmt` | Formats configuration files to canonical style |
| `terraform output` | Shows output values from state file |
| `terraform graph` | Generates a visual dependency graph |
| `terraform refresh` | Updates state file with real infrastructure |

### Q5: How does Terraform work?

**Answer:**
Terraform follows a straightforward workflow :

1. **Write**: Define infrastructure in `.tf` configuration files using HCL
2. **Plan**: Run `terraform plan` to preview changes without applying them
3. **Apply**: Execute `terraform apply` to provision the infrastructure
4. **Manage**: Update configurations and reapply as needed
5. **Destroy**: Clean up resources with `terraform destroy` when no longer needed

Terraform creates a **dependency graph** from configurations, determines the correct order for resource creation, and makes API calls to providers to provision resources .

### Q6: What is a Terraform provider?

**Answer:**
A **Terraform provider** is a plugin that enables Terraform to interact with external APIs and services . Each provider handles the lifecycle of resources within a specific service (AWS, Azure, GCP, Kubernetes, etc.).

**Provider configuration example**:
```hcl
provider "aws" {
  region = "us-west-2"
}

provider "azurerm" {
  features {}
}
```

**Common providers**:
- AWS
- AzureRM
- Google Cloud Platform
- Kubernetes
- Helm
- GitHub
- Datadog 

### Q7: What is a Terraform resource?

**Answer:**
A **resource** in Terraform is a component of your infrastructure that you want to manage . Resources can be virtual machines, databases, DNS records, load balancers, or any other infrastructure object supported by providers.

**Resource syntax**:
```hcl
resource "aws_instance" "web_server" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
  
  tags = {
    Name = "Web Server"
  }
}
```

### Q8: What is the Terraform state file?

**Answer:**
The **Terraform state file** (typically `terraform.tfstate`) is a JSON file that maps the resources defined in your configuration to the real-world infrastructure . It's critical for Terraform's operation because:

- **Tracks metadata**: Stores resource IDs, attributes, and dependencies
- **Enables updates**: Compares desired vs. current state to determine changes
- **Improves performance**: Avoids querying all resources on every plan
- **Facilitates collaboration**: Allows teams to work on same infrastructure 

**Important**: State files may contain sensitive information and should be stored securely, preferably in remote backends with encryption.

### Q9: What is `terraform init` and why is it necessary?

**Answer:**
`terraform init` is the first command to run after writing a new Terraform configuration . It performs several critical functions:

1. **Downloads provider plugins** specified in the configuration
2. **Initializes backend** configuration for state storage
3. **Installs modules** referenced in the configuration
4. **Creates `.terraform` directory** with necessary files

It's safe to run multiple times and is required before any other Terraform commands .

### Q10: What is the difference between `terraform plan` and `terraform apply`?

**Answer:**
| Command | Purpose | Safety |
|---------|---------|--------|
| **`terraform plan`** | Creates an execution plan showing what changes will be made without actually changing anything  | Read-only, safe to run anytime |
| **`terraform apply`** | Executes the changes to reach the desired state, creating, modifying, or destroying resources  | Makes actual changes, requires confirmation |

**Best practice**: Always run `terraform plan` first to review changes before applying them.

---

## Intermediate Terraform Questions

### Q11: What are Terraform modules and why use them?

**Answer:**
**Terraform modules** are reusable, encapsulated collections of Terraform configuration files that manage related resources together . They're the primary way to organize and reuse infrastructure code.

**Module types**:
- **Root module**: The main directory where you run Terraform commands
- **Child module**: A module called by another configuration 

**Benefits** :
- **Reusability**: Write once, use across multiple environments
- **Organization**: Break complex infrastructure into manageable pieces
- **Consistency**: Ensure same configurations across teams
- **Versioning**: Track and control module versions

**Module structure**:
```
modules/
  â”œâ”€â”€ networking/
  â”‚   â”œâ”€â”€ main.tf
  â”‚   â”œâ”€â”€ variables.tf
  â”‚   â””â”€â”€ outputs.tf
  â””â”€â”€ compute/
      â”œâ”€â”€ main.tf
      â”œâ”€â”€ variables.tf
      â””â”€â”€ outputs.tf
```

### Q12: What are Terraform variables and how are they used?

**Answer:**
**Terraform variables** (input variables) parameterize your configurations, making them reusable and adaptable to different environments .

**Variable types**:
```hcl
# String variable
variable "environment" {
  description = "Deployment environment"
  type        = string
  default     = "development"
}

# List variable
variable "availability_zones" {
  type    = list(string)
  default = ["us-west-1a", "us-west-1b"]
}

# Map variable
variable "instance_types" {
  type = map(string)
  default = {
    dev  = "t2.micro"
    prod = "t2.large"
  }
}

# Sensitive variable
variable "db_password" {
  description = "Database password"
  type        = string
  sensitive   = true
}
```

**Ways to set variables** :
1. Default values in variable blocks
2. Command-line: `-var="environment=production"`
3. Variable files: `terraform.tfvars` or `*.auto.tfvars`
4. Environment variables: `TF_VAR_environment`
5. CI/CD pipeline variables

### Q13: What are Terraform outputs?

**Answer:**
**Output values** are the return values of a Terraform module that can be queried by users or passed to other configurations .

**Use cases**:
- Display generated information (IP addresses, DNS names)
- Share data between modules
- Expose values for other systems to consume

**Output example**:
```hcl
output "instance_ip" {
  description = "Public IP of the EC2 instance"
  value       = aws_instance.web.public_ip
  sensitive   = false
}

output "database_connection_string" {
  description = "Connection string for database"
  value       = aws_db_instance.main.connection_string
  sensitive   = true
}
```

View outputs with: `terraform output` 

### Q14: What is a Terraform backend?

**Answer:**
A **Terraform backend** determines where Terraform stores its state file . Backends can be local (default) or remote.

**Types**:
- **Local backend**: Stores state on local disk (default)
- **Remote backends**: Store state in remote, shared locations 

**Common remote backends**:
- Amazon S3
- Azure Storage Account
- Google Cloud Storage
- HashiCorp Terraform Cloud/Enterprise
- HTTP backend

**Backend configuration example**:
```hcl
terraform {
  backend "s3" {
    bucket         = "my-company-terraform-state"
    key            = "prod/network/terraform.tfstate"
    region         = "us-west-2"
    dynamodb_table = "terraform-state-lock"
    encrypt        = true
  }
}
```

**Benefits of remote backends** :
- Team collaboration
- State locking to prevent conflicts
- Versioning and history
- Enhanced security

### Q15: Explain state file locking

**Answer:**
**State file locking** is a mechanism that prevents multiple users or processes from modifying the Terraform state file simultaneously . This prevents conflicts and corruption.

**How it works**:
1. When a user runs `terraform apply`, Terraform acquires a lock
2. The lock prevents other operations from modifying state
3. After completion, Terraform releases the lock
4. If another user attempts to run during this time, they'll receive an error

**Supported backends**:
- AWS S3 + DynamoDB
- Azure Storage Account + Lease
- Google Cloud Storage + Object holds
- Terraform Cloud/Enterprise (built-in)

**Force unlock** (when locks fail):
```bash
terraform force-unlock LOCK_ID [DIR]
```
Use with cautionâ€”only when you're certain the locking process has failed .

### Q16: What is a tainted resource?

**Answer:**
A **tainted resource** in Terraform is a resource that has been marked for recreation during the next `terraform apply` . This is useful when a resource is in a failed or degraded state but its configuration hasn't changed.

**Mark resource as tainted**:
```bash
terraform taint aws_instance.web_server
```

**Remove taint**:
```bash
terraform untaint aws_instance.web_server
```

**What happens**:
- Tainted resource will be destroyed and recreated on next apply
- This ensures the resource matches the desired state
- Dependencies are handled automatically 

### Q17: How do you handle sensitive data in Terraform?

**Answer:**
Terraform provides several mechanisms to handle sensitive data securely :

**1. Sensitive variables**:
```hcl
variable "db_password" {
  description = "Database password"
  type        = string
  sensitive   = true
}
```

**2. Avoid hardcoding**:
- Never store secrets in plain text in configuration files
- Use variable files that are not committed to version control

**3. Environment variables**:
```bash
export TF_VAR_db_password="my-secure-password"
```

**4. External secret management**:
- HashiCorp Vault
- AWS Secrets Manager
- Azure Key Vault
- GCP Secret Manager

**5. Terraform Cloud/Enterprise**:
- Store variables as "sensitive" in workspaces

**6. Tools for encryption**:
- `terrahelp` for encrypting variables and state files 
- SOPS (Secrets OPerationS)

### Q18: What are data sources in Terraform?

**Answer:**
**Data sources** allow Terraform to read information from resources defined outside of Terraform or in a different Terraform configuration . They enable fetching and computing data for use in configurations.

**Use cases**:
- Fetch the latest AMI ID for an AWS instance
- Read an existing VPC ID
- Get current user information
- Look up DNS records

**Example**:
```hcl
# Get the latest Ubuntu AMI
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]
  
  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

# Use the data source
resource "aws_instance" "web" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t3.micro"
}
```

### Q19: What are provisioners and when should you use them?

**Answer:**
**Provisioners** are used to execute scripts or configuration management tools on local or remote machines after resource creation . They're considered a last resort and should be used sparingly.

**Types of provisioners** :
- **file**: Copy files to the resource
- **remote-exec**: Execute scripts on remote resource via SSH/WinRM
- **local-exec**: Execute scripts on the machine running Terraform
- **chef**, **puppet**, **salt-masterless**: Configuration management integrations

**Example**:
```hcl
resource "aws_instance" "web" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
  
  provisioner "remote-exec" {
    inline = [
      "sudo apt-get update",
      "sudo apt-get install -y nginx",
      "sudo systemctl start nginx"
    ]
    
    connection {
      type        = "ssh"
      user        = "ubuntu"
      private_key = file("~/.ssh/id_rsa")
      host        = self.public_ip
    }
  }
}
```

**Best practices** :
- Use configuration management tools (Ansible, Chef) instead
- Consider immutable infrastructure patterns
- Use provisioners only for simple bootstrapping

### Q20: What is the `null_resource` and when is it useful?

**Answer:**
The **null_resource** is a placeholder resource that doesn't create any real infrastructure but follows the standard resource lifecycle . It's used to trigger provisioners or organize dependencies.

**Common use cases** :
- Running scripts at specific points in the resource graph
- Creating dependencies between unrelated resources
- Triggering actions based on changes to other resources

**Example**:
```hcl
resource "null_resource" "setup" {
  triggers = {
    always_run = timestamp()
  }
  
  provisioner "local-exec" {
    command = "echo 'Triggered at ${timestamp()}' >> log.txt"
  }
  
  depends_on = [aws_instance.web]
}
```

---

## Advanced Terraform Questions

### Q21: Explain the difference between `count` and `for_each`

**Answer:**
Both `count` and `for_each` create multiple instances of a resource, but they work differently :

| Aspect | `count` | `for_each` |
|--------|---------|------------|
| **Input type** | Number (integer) | Map or set of strings |
| **Access** | `count.index` | `each.key`, `each.value` |
| **Resource addressing** | `resource.name[0]` | `resource.name["key"]` |
| **Handles reordering** | Can cause unexpected changes | More stable with unordered data |

**`count` example**:
```hcl
variable "subnet_cidrs" {
  type    = list(string)
  default = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
}

resource "aws_subnet" "main" {
  count             = length(var.subnet_cidrs)
  vpc_id            = aws_vpc.main.id
  cidr_block        = var.subnet_cidrs[count.index]
  availability_zone = data.aws_availability_zones.available.names[count.index]
}
```

**`for_each` example**:
```hcl
variable "instances" {
  type = map(object({
    ami           = string
    instance_type = string
  }))
  default = {
    web = {
      ami           = "ami-0c55b159cbfafe1f0"
      instance_type = "t2.micro"
    }
    app = {
      ami           = "ami-0c55b159cbfafe1f0"
      instance_type = "t2.small"
    }
  }
}

resource "aws_instance" "this" {
  for_each      = var.instances
  ami           = each.value.ami
  instance_type = each.value.instance_type
  
  tags = {
    Name = each.key
  }
}
```

### Q22: What are dynamic blocks?

**Answer:**
**Dynamic blocks** allow you to construct repeatable nested blocks inside resources, data sources, and other constructs . They're useful when you need to generate multiple similar blocks based on a variable.

**Example without dynamic blocks** (repetitive):
```hcl
resource "aws_security_group" "main" {
  name = "web-sg"
  
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["10.0.0.0/8"]
  }
}
```

**With dynamic blocks**:
```hcl
variable "ingress_rules" {
  type = list(object({
    port        = number
    protocol    = string
    cidr_blocks = list(string)
  }))
  default = [
    { port = 80, protocol = "tcp", cidr_blocks = ["0.0.0.0/0"] },
    { port = 443, protocol = "tcp", cidr_blocks = ["0.0.0.0/0"] },
    { port = 22, protocol = "tcp", cidr_blocks = ["10.0.0.0/8"] }
  ]
}

resource "aws_security_group" "main" {
  name = "web-sg"
  
  dynamic "ingress" {
    for_each = var.ingress_rules
    content {
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = ingress.value.protocol
      cidr_blocks = ingress.value.cidr_blocks
    }
  }
}
```

### Q23: What are Terraform workspaces?

**Answer:**
**Terraform workspaces** provide a way to manage multiple distinct instances of the same configuration . They're commonly used to create separate environments (dev, staging, prod) without duplicating code.

**Workspace commands**:
```bash
# List workspaces
terraform workspace list

# Create and switch to new workspace
terraform workspace new dev
terraform workspace new staging
terraform workspace new prod

# Switch workspace
terraform workspace select staging

# Show current workspace
terraform workspace show
```

**How they work**:
- Each workspace has its own state file
- Resources are tracked separately per workspace
- Can access current workspace name with `${terraform.workspace}`

**Example with workspaces**:
```hcl
resource "aws_instance" "web" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = terraform.workspace == "prod" ? "t3.large" : "t3.micro"
  
  tags = {
    Environment = terraform.workspace
    Name        = "web-${terraform.workspace}"
  }
}
```

**Limitations** :
- Not suitable for managing complex state hierarchies
- For advanced use cases, consider separate directory structures or Terragrunt

### Q24: What is Terragrunt and why use it?

**Answer:**
**Terragrunt** is a thin wrapper for Terraform that provides additional tools for keeping Terraform configurations DRY (Don't Repeat Yourself), managing remote state, and working with multiple Terraform modules .

**Key features** :
- **DRY configuration**: Define common variables once and reuse
- **Remote state management**: Automatic state file configuration
- **Module versioning**: Easily lock and update module versions
- **CLI enhancements**: Execute commands across multiple modules
- **Dependency management**: Define dependencies between modules

**Example Terragrunt structure**:
```
terragrunt/
  â”œâ”€â”€ terragrunt.hcl
  â”œâ”€â”€ dev/
  â”‚   â”œâ”€â”€ terragrunt.hcl
  â”‚   â”œâ”€â”€ vpc/
  â”‚   â”‚   â””â”€â”€ terragrunt.hcl
  â”‚   â””â”€â”€ ecs/
  â”‚       â””â”€â”€ terragrunt.hcl
  â””â”€â”€ prod/
      â”œâ”€â”€ terragrunt.hcl
      â”œâ”€â”€ vpc/
      â”‚   â””â”€â”€ terragrunt.hcl
      â””â”€â”€ ecs/
          â””â”€â”€ terragrunt.hcl
```

**Terragrunt HCL example**:
```hcl
# Root terragrunt.hcl
terraform {
  source = "git::git@github.com:company/infrastructure-modules.git//networking/vpc?ref=v1.2.0"
}

inputs = {
  environment = "dev"
  vpc_cidr    = "10.0.0.0/16"
  region      = "us-west-2"
}

remote_state {
  backend = "s3"
  config = {
    bucket         = "company-terraform-state"
    key            = "${path_relative_to_include()}/terraform.tfstate"
    region         = "us-west-2"
    encrypt        = true
    dynamodb_table = "terraform-locks"
  }
}
```

### Q25: How do you handle dependency management between modules?

**Answer:**
Terraform handles dependencies through **explicit** and **implicit** relationships :

**1. Implicit dependencies** (automatic):
```hcl
# Terraform automatically knows subnet depends on vpc
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"
}

resource "aws_subnet" "main" {
  vpc_id     = aws_vpc.main.id
  cidr_block = "10.0.1.0/24"
}
```

**2. Explicit dependencies with `depends_on`**:
```hcl
resource "aws_instance" "web" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
  
  depends_on = [
    aws_s3_bucket.logs,
    aws_iam_role.instance_role
  ]
}
```

**3. Module dependencies**:
```hcl
module "vpc" {
  source = "./modules/vpc"
  cidr   = "10.0.0.0/16"
}

module "web" {
  source      = "./modules/web"
  vpc_id      = module.vpc.vpc_id
  subnet_ids  = module.vpc.public_subnet_ids
  
  depends_on = [module.vpc]  # Explicit dependency
}
```

**4. Data source dependencies**:
```hcl
# This won't be resolved until after vpc is created
data "aws_vpc" "selected" {
  id = aws_vpc.main.id
}
```

### Q26: Explain the `terraform refresh` command

**Answer:**
`terraform refresh` is a command that updates the Terraform state file to match the real-world infrastructure . It queries providers for the current state of resources and updates the state file accordingly.

**Important points**:
- Does **not** change infrastructure
- Updates only the state file
- Typically runs automatically during `plan` and `apply`
- Useful when infrastructure was modified outside of Terraform

**When to use manually**:
- After manual changes to infrastructure
- When recovering from state file corruption
- Before importing existing resources

**Command**:
```bash
terraform refresh
```

### Q27: What is the import command and how is it used?

**Answer:**
`terraform import` brings existing infrastructure under Terraform management without recreating it . This is essential for adopting Terraform in brownfield environments.

**Syntax**:
```bash
terraform import [options] ADDRESS ID
```

- **ADDRESS**: Terraform resource address (e.g., `aws_instance.web`)
- **ID**: Provider-specific resource identifier

**Example**:
```bash
# Import existing EC2 instance
terraform import aws_instance.web i-1234567890abcdef0
```

**Steps for importing**:
1. Write resource configuration matching the existing infrastructure
2. Run `terraform import` with appropriate address and ID
3. Run `terraform plan` to verify configuration matches reality
4. Update configuration if needed based on plan output
5. Manage normally going forward

**Best practices** :
- Always verify with `terraform plan` after import
- Update configuration to match actual resource attributes
- Consider using `terraform show` to see imported resource details

### Q28: What are Sentinel policies?

**Answer:**
**Sentinel** is HashiCorp's policy-as-code framework that enables fine-grained, logic-based policy decisions . It's available in Terraform Enterprise and Cloud.

**Enforcement levels** :
- **Advisory**: Logs violations but allows operation to continue
- **Soft mandatory**: Requires override approval for violations
- **Hard mandatory**: Blocks operations entirely

**Common policy examples** :
- Enforce explicit resource ownership tags
- Restrict which cloud providers or regions can be used
- Mandate resource tagging standards
- Limit allowed instance types
- Enforce encryption requirements
- Prohibit certain resource types

**Example Sentinel policy**:
```python
# Require all resources to have Environment tag
import "tfresources"

all_aws_instances = tfresources.resources("aws_instance", "aws_launch_configuration")

violating_instances = filter all_aws_instances as _, instance {
    not instance.tags else contains_keys(instance.tags, ["Environment"])
}

main = rule {
    length(violating_instances) is 0
}
```

### Q29: How do you perform blue-green deployments with Terraform?

**Answer:**
**Blue-green deployment** is a strategy that reduces downtime and risk by running two identical production environments (blue and green) and switching traffic between them .

**Implementation approach**:

```hcl
# variables.tf
variable "active_color" {
  description = "Which environment is active (blue or green)"
  type        = string
  default     = "blue"
}

# main.tf
resource "aws_instance" "blue" {
  count         = var.active_color == "blue" ? 1 : 0
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
  
  user_data = <<-EOF
    #!/bin/bash
    echo "BLUE ENVIRONMENT" > /var/www/html/index.html
    systemctl start nginx
  EOF
  
  tags = {
    Name    = "blue-env"
    Color   = "blue"
    Active  = var.active_color == "blue" ? "true" : "false"
  }
}

resource "aws_instance" "green" {
  count         = var.active_color == "green" ? 1 : 0
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
  
  user_data = <<-EOF
    #!/bin/bash
    echo "GREEN ENVIRONMENT" > /var/www/html/index.html
    systemctl start nginx
  EOF
  
  tags = {
    Name    = "green-env"
    Color   = "green"
    Active  = var.active_color == "green" ? "true" : "false"
  }
}

# Load balancer target groups
resource "aws_lb_target_group" "blue" {
  name     = "blue-tg"
  port     = 80
  protocol = "HTTP"
  vpc_id   = aws_vpc.main.id
}

resource "aws_lb_target_group" "green" {
  name     = "green-tg"
  port     = 80
  protocol = "HTTP"
  vpc_id   = aws_vpc.main.id
}

resource "aws_lb_listener" "main" {
  load_balancer_arn = aws_lb.main.arn
  port              = "80"
  protocol          = "HTTP"
  
  default_action {
    type             = "forward"
    target_group_arn = var.active_color == "blue" ? aws_lb_target_group.blue.arn : aws_lb_target_group.green.arn
  }
}
```

**Deployment steps**:
1. Deploy new version to inactive environment
2. Test thoroughly
3. Switch `active_color` variable and reapply
4. Monitor for issues
5. Remove old environment after verification

### Q30: How do you manage rollbacks in Terraform?

**Answer:**
Rollbacks in Terraform can be managed through several strategies :

**1. Version control rollback**:
```bash
# Revert to previous configuration in Git
git checkout <previous-commit-hash>
terraform plan
terraform apply
```

**2. State file recovery**:
```bash
# If using remote backend with versioning
# Restore previous state file version from S3/GCS/Azure
# Then apply that state
terraform init -reconfigure
terraform apply
```

**3. Terraform Enterprise/Cloud features** :
- Built-in state versioning
- Run history with ability to re-run
- State rollback feature for corrupted states

**4. Manual state manipulation** (last resort):
```bash
# List available state versions
terraform state list

# Remove problematic resources
terraform state rm aws_instance.problematic

# Re-import if needed
terraform import aws_instance.problematic i-1234567890
```

**Best practices** :
- Always version control configurations
- Enable state file versioning in backend
- Test rollback procedures in non-production
- Document and practice disaster recovery

---

## Terraform Scenario-Based Questions

### Q31: Managing Multiple Environments

**Scenario:** Your organization needs to manage three environments (dev, staging, production) with similar infrastructure but different configurations.

**Question:** How would you structure your Terraform code to handle this efficiently?

**Answer:**
Multiple approaches exist, each with trade-offs:

**Option 1: Directory structure with separate configurations**:
```
environments/
  â”œâ”€â”€ dev/
  â”‚   â”œâ”€â”€ main.tf
  â”‚   â”œâ”€â”€ variables.tf
  â”‚   â”œâ”€â”€ terraform.tfvars
  â”‚   â””â”€â”€ backend.tf
  â”œâ”€â”€ staging/
  â”‚   â”œâ”€â”€ main.tf
  â”‚   â”œâ”€â”€ variables.tf
  â”‚   â”œâ”€â”€ terraform.tfvars
  â”‚   â””â”€â”€ backend.tf
  â””â”€â”€ prod/
      â”œâ”€â”€ main.tf
      â”œâ”€â”€ variables.tf
      â”œâ”€â”€ terraform.tfvars
      â””â”€â”€ backend.tf
modules/
  â”œâ”€â”€ networking/
  â”œâ”€â”€ compute/
  â””â”€â”€ database/
```

**Option 2: Workspace-based approach**:
```hcl
# main.tf
terraform {
  backend "s3" {
    bucket = "company-terraform-state"
    key    = "environments/${terraform.workspace}/terraform.tfstate"
    region = "us-west-2"
  }
}

variable "environment_config" {
  type = map(object({
    instance_type = string
    instance_count = number
    vpc_cidr = string
  }))
  default = {
    dev = {
      instance_type  = "t3.micro"
      instance_count = 1
      vpc_cidr       = "10.0.0.0/16"
    }
    staging = {
      instance_type  = "t3.small"
      instance_count = 2
      vpc_cidr       = "10.1.0.0/16"
    }
    prod = {
      instance_type  = "t3.large"
      instance_count = 5
      vpc_cidr       = "10.2.0.0/16"
    }
  }
}

locals {
  env_config = var.environment_config[terraform.workspace]
}

resource "aws_instance" "app" {
  count         = local.env_config.instance_count
  instance_type = local.env_config.instance_type
  # ... other configuration
}
```

**Option 3: Terragrunt for DRY configurations**:

**Recommended approach** : Directory structure with shared modules and environment-specific tfvars files. This provides:
- Clear separation of concerns
- Independent state files per environment
- Ability to apply changes to specific environments only
- Easier access control per environment

### Q32: Recovering from State File Corruption

**Scenario:** Your Terraform state file has become corrupted or out of sync with actual infrastructure.

**Question:** What steps would you take to recover?

**Answer:**
Systematic recovery approach :

**Step 1: Assess the damage**
```bash
# Try to view state
terraform state list
# If this fails, proceed to recovery
```

**Step 2: Use state backups**
```bash
# If using remote backend with versioning
# Restore previous version manually from S3/GCS/Azure
# Then reinitialize
terraform init -reconfigure
terraform plan  # Verify state matches reality
```

**Step 3: Import resources individually** (if state is completely lost)
```bash
# First, write configuration matching existing resources
# Then import each resource
terraform import aws_instance.web i-1234567890abcdef0
terraform import aws_security_group.web_sg sg-12345678
# ... continue for all resources
```

**Step 4: Use state manipulation commands**
```bash
# Remove problematic resources
terraform state rm aws_instance.corrupted

# If resources exist but not in state, use terraform refresh
terraform refresh  # Updates state to match reality
```

**Step 5: Verify and test**
```bash
# After recovery, verify everything
terraform plan
# Should show no changes if state matches infrastructure
```

**Prevention measures**:
- Always use remote backends with versioning
- Enable state locking
- Regular backups
- Version control for configurations

### Q33: Handling Provider Authentication

**Scenario:** You need to manage infrastructure across AWS, Azure, and GCP with different authentication methods.

**Question:** How do you handle provider credentials securely across environments?

**Answer:**
Multiple secure approaches :

**1. Environment variables** (recommended for CI/CD):
```bash
# AWS
export AWS_ACCESS_KEY_ID="AKIA..."
export AWS_SECRET_ACCESS_KEY="..."
export AWS_DEFAULT_REGION="us-west-2"

# Azure
export ARM_SUBSCRIPTION_ID="..."
export ARM_CLIENT_ID="..."
export ARM_CLIENT_SECRET="..."
export ARM_TENANT_ID="..."

# GCP
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
```

**2. Provider configuration with variables**:
```hcl
# variables.tf
variable "aws_access_key" {
  description = "AWS Access Key"
  type        = string
  sensitive   = true
}

variable "aws_secret_key" {
  description = "AWS Secret Key"
  type        = string
  sensitive   = true
}

# main.tf
provider "aws" {
  region     = var.aws_region
  access_key = var.aws_access_key
  secret_key = var.aws_secret_key
}
```

**3. Terraform Cloud/Enterprise variable sets**:
- Store credentials as sensitive variables
- Assign variable sets to workspaces
- Automatic injection during runs

**4. HashiCorp Vault integration**:
```hcl
# Using Vault provider to fetch dynamic credentials
data "vault_aws_access_credentials" "creds" {
  backend = "aws"
  role    = "my-role"
}

provider "aws" {
  region     = "us-west-2"
  access_key = data.vault_aws_access_credentials.creds.access_key
  secret_key = data.vault_aws_access_credentials.creds.secret_key
}
```

**5. Workload identity federation** (cloud-native):
- AWS IAM Roles for Service Accounts (IRSA)
- Azure AD Workload Identity
- GCP Workload Identity Federation

### Q34: Terraform in CI/CD Pipeline

**Scenario:** You need to integrate Terraform into a CI/CD pipeline for automated infrastructure deployments.

**Question:** How would you design a secure and efficient CI/CD pipeline for Terraform?

**Answer:**
A comprehensive CI/CD pipeline for Terraform :

**GitLab CI example**:
```yaml
# .gitlab-ci.yml
stages:
  - validate
  - plan
  - apply

variables:
  TF_ROOT: ${CI_PROJECT_DIR}
  TF_IN_AUTOMATION: "true"

cache:
  key: "${CI_COMMIT_REF_SLUG}"
  paths:
    - ${TF_ROOT}/.terraform

before_script:
  - cd ${TF_ROOT}
  - terraform --version
  - terraform init

validate:
  stage: validate
  script:
    - terraform validate
    - terraform fmt -check

plan:
  stage: plan
  script:
    - terraform plan -out=plan.tfplan
  artifacts:
    paths:
      - ${TF_ROOT}/plan.tfplan
  only:
    - merge_requests
    - main
    - develop

apply:
  stage: apply
  script:
    - terraform apply -auto-approve plan.tfplan
  when: manual
  only:
    - main
    - develop
```

**GitHub Actions example**:
```yaml
name: 'Terraform'

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  terraform:
    name: 'Terraform'
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout
      uses: actions/checkout@v2
      
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v1
      
    - name: Terraform Format
      run: terraform fmt -check
      
    - name: Terraform Init
      run: terraform init
      
    - name: Terraform Validate
      run: terraform validate
      
    - name: Terraform Plan
      run: terraform plan
      
    - name: Terraform Apply
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: terraform apply -auto-approve
```

**Jenkins pipeline example**:
```groovy
pipeline {
    agent any
    
    environment {
        TF_IN_AUTOMATION = "true"
    }
    
    stages {
        stage('Checkout') {
            steps { checkout scm }
        }
        
        stage('Terraform Init') {
            steps {
                sh 'terraform init'
            }
        }
        
        stage('Terraform Validate') {
            steps {
                sh 'terraform validate'
                sh 'terraform fmt -check'
            }
        }
        
        stage('Terraform Plan') {
            steps {
                sh 'terraform plan -out=plan.tfplan'
            }
            post {
                success {
                    archiveArtifacts 'plan.tfplan'
                }
            }
        }
        
        stage('Approval') {
            when {
                branch 'main'
            }
            input {
                message "Apply Terraform changes to production?"
                ok "Yes"
            }
        }
        
        stage('Terraform Apply') {
            when {
                branch 'main'
            }
            steps {
                sh 'terraform apply -auto-approve plan.tfplan'
            }
        }
    }
}
```

**Best practices** :
- Always run `terraform plan` in merge/pull requests
- Use `-auto-approve` only in CI/CD, never locally
- Store state remotely with locking
- Handle secrets via CI/CD variables
- Implement approval gates for production
- Use separate service accounts for CI/CD

### Q35: Refactoring Large Terraform Configurations

**Scenario:** You have a monolithic Terraform configuration that's become difficult to maintain and slow to run.

**Question:** How would you refactor it into a more manageable structure?

**Answer:**
Systematic refactoring approach :

**Step 1: Analyze current state**
```bash
# Understand dependencies
terraform graph > graph.dot

# List all resources
terraform state list
```

**Step 2: Identify logical groupings**
- Group related resources (networking, compute, database, etc.)
- Consider team ownership boundaries
- Identify shared vs. environment-specific resources

**Step 3: Create module structure**
```
terraform-repo/
  â”œâ”€â”€ modules/
  â”‚   â”œâ”€â”€ networking/
  â”‚   â”‚   â”œâ”€â”€ main.tf
  â”‚   â”‚   â”œâ”€â”€ variables.tf
  â”‚   â”‚   â””â”€â”€ outputs.tf
  â”‚   â”œâ”€â”€ compute/
  â”‚   â”‚   â”œâ”€â”€ main.tf
  â”‚   â”‚   â”œâ”€â”€ variables.tf
  â”‚   â”‚   â””â”€â”€ outputs.tf
  â”‚   â””â”€â”€ database/
  â”‚       â”œâ”€â”€ main.tf
  â”‚       â”œâ”€â”€ variables.tf
  â”‚       â””â”€â”€ outputs.tf
  â”œâ”€â”€ environments/
  â”‚   â”œâ”€â”€ dev/
  â”‚   â”‚   â”œâ”€â”€ main.tf
  â”‚   â”‚   â”œâ”€â”€ terraform.tfvars
  â”‚   â”‚   â””â”€â”€ backend.tf
  â”‚   â””â”€â”€ prod/
  â”‚       â”œâ”€â”€ main.tf
  â”‚       â”œâ”€â”€ terraform.tfvars
  â”‚       â””â”€â”€ backend.tf
  â””â”€â”€ shared/
      â””â”€â”€ main.tf  # Shared resources (VPC, etc.)
```

**Step 4: Use state mv for migration without recreation**
```bash
# Move resources to new addresses
terraform state mv aws_instance.web_old module.compute.aws_instance.web

# Move resources to new state files
terraform state mv -state-out=environments/dev/terraform.tfstate \
  aws_vpc.main module.vpc.aws_vpc.main
```

**Step 5: Implement remote state data sources**
```hcl
# environments/dev/main.tf
data "terraform_remote_state" "shared" {
  backend = "s3"
  config = {
    bucket = "company-terraform-state"
    key    = "shared/terraform.tfstate"
    region = "us-west-2"
  }
}

module "compute" {
  source   = "../../modules/compute"
  vpc_id   = data.terraform_remote_state.shared.outputs.vpc_id
  subnet_ids = data.terraform_remote_state.shared.outputs.subnet_ids
}
```

**Step 6: Test incrementally**
- Start with non-critical resources
- Verify each step with `terraform plan`
- Maintain ability to rollback
- Document the new structure

**Best practices** :
- Make one change at a time
- Use version control branches
- Test in non-production first
- Update documentation
- Train team on new structure

---

## Terraform Commands Cheat Sheet

| Category | Command | Description |
|----------|---------|-------------|
| **Lifecycle** | `terraform init` | Initialize working directory, download plugins, setup backend  |
| | `terraform plan` | Show execution plan (preview changes)  |
| | `terraform apply` | Apply changes to reach desired state  |
| | `terraform destroy` | Destroy all managed resources  |
| | `terraform refresh` | Update state to match real infrastructure  |
| **State Management** | `terraform state list` | List resources in state  |
| | `terraform state show` | Show details of a resource in state |
| | `terraform state mv` | Move items in state (rename/restructure) |
| | `terraform state rm` | Remove items from state  |
| | `terraform state pull` | Download state from remote backend |
| | `terraform state push` | Upload local state to remote backend |
| **Resource Management** | `terraform taint` | Mark resource for recreation  |
| | `terraform untaint` | Remove taint from resource |
| | `terraform import` | Import existing infrastructure  |
| | `terraform output` | Show output values  |
| **Validation & Formatting** | `terraform validate` | Check configuration for validity  |
| | `terraform fmt` | Format configuration files  |
| **Workspace** | `terraform workspace list` | List workspaces  |
| | `terraform workspace new` | Create new workspace  |
| | `terraform workspace select` | Switch workspace  |
| | `terraform workspace show` | Show current workspace  |
| **Graph & Debug** | `terraform graph` | Generate dependency graph  |
| | `terraform version` | Show Terraform version  |
| | `terraform force-unlock` | Manually unlock state  |
| | `terraform providers` | Show provider requirements |
| **Advanced** | `terraform plan -out=file` | Save plan to file  |
| | `terraform apply -target=resource` | Apply specific resource only  |
| | `terraform apply -var='key=value'` | Set variable from command line |
| | `terraform apply -auto-approve` | Skip approval prompt (CI/CD) |
| | `terraform apply -refresh=false` | Skip refresh before apply |

---

## ðŸ’¡ Tips for Terraform Interview Success

1. **Understand IaC fundamentals** - Be clear on declarative vs. imperative approaches and why Terraform's declarative model wins 

2. **Master state management** - This is where most candidates fail. Understand state files, locking, backends, and remote state 

3. **Know the ecosystem** - Terraform Cloud, Enterprise, Sentinel, Terragrunt, and integrations with CI/CD 

4. **Practice with real scenarios** - Create test configurations, break things, and fix them. Hands-on experience beats theory 

5. **Security mindset** - Know how to handle secrets, use Vault, and implement least-privilege access 

6. **Module design patterns** - Understand how to create reusable, well-documented modules 

7. **Troubleshooting approach** - Be ready to discuss how you'd debug failed applies, state corruption, or provider issues 

8. **Stay current** - Know about recent versions (1.8.x features), provider updates, and best practices 

9. **Real-world stories** - Be prepared to answer: "What's the biggest Terraform disaster you've seen, and how did you fix it?" 

10. **Cross-cloud knowledge** - Understand how Terraform works across AWS, Azure, GCP, and on-premises 

Good luck with your Terraform interview! ðŸš€