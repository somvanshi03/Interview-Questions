# Kubernetes (K8s) interview questions 

---

## ðŸ“‹ Table of Contents
- [Basic Kubernetes Questions](#basic-kubernetes-questions)
- [Intermediate Kubernetes Questions](#intermediate-kubernetes-questions)
- [Advanced Kubernetes Questions](#advanced-kubernetes-questions)
- [Kubernetes Scenario-Based Questions](#kubernetes-scenario-based-questions)
- [Kubernetes Commands Cheat Sheet](#kubernetes-commands-cheat-sheet)

---

## Basic Kubernetes Questions

### Q1: What is Kubernetes and why is it used?

**Answer:**
Kubernetes (often abbreviated as **K8s**) is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications . Originally developed by Google, it's now maintained by the Cloud Native Computing Foundation (CNCF) .

**Why it's used**:
- **Automation**: Automates container deployment, replication, and scaling 
- **Self-healing**: Automatically restarts failed containers, replaces unresponsive nodes, and reschedules workloads 
- **Load balancing**: Distributes traffic efficiently between containers 
- **Scalability**: Enables both horizontal and vertical scaling based on demand 
- **Portability**: Works across all cloud providers and on-premises environments 

### Q2: What is a Pod in Kubernetes?

**Answer:**
A **Pod** is the smallest and simplest deployable unit in Kubernetes . It represents a single instance of a running process in your cluster and encapsulates one or more containers, storage resources, a unique network IP, and options that govern how the containers should run .

**Key characteristics**:
- Containers within a Pod share the same network namespace (can communicate via localhost) 
- They share storage volumes 
- Pods are ephemeralâ€”they can be created, destroyed, and replaced dynamically 
- All containers in a Pod are scheduled together on the same node 

**Example Pod YAML**:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: nginx-container
    image: nginx:1.21
    ports:
    - containerPort: 80
```

### Q3: What is a Node in Kubernetes?

**Answer:**
A **Node** is a worker machine in Kubernetes, which can be either a virtual machine (VM) or a physical server . Nodes are the workhorses that run your containerized applications .

**Components running on each Node**:
- **Kubelet**: An agent that ensures containers are running as expected in a Pod 
- **Kube-proxy**: Maintains network rules and enables communication between services and Pods 
- **Container runtime**: The software that runs containers (e.g., containerd, CRI-O, or Docker) 

### Q4: What are the main components of the Kubernetes Control Plane (Master Node)?

**Answer:**
The **Control Plane** (formerly called the Master Node) manages the Kubernetes cluster and makes global decisions . Its key components are :

| Component | Responsibility |
|-----------|----------------|
| **kube-apiserver** | Front-end for the control plane; exposes the Kubernetes API and handles all REST requests  |
| **etcd** | Consistent, highly-available key-value store for all cluster data (configuration, state, metadata)  |
| **kube-scheduler** | Watches for new Pods and assigns them to appropriate nodes based on resource availability  |
| **kube-controller-manager** | Runs controller processes (Node Controller, Replication Controller, etc.) to maintain desired state  |
| **cloud-controller-manager** | (Optional) Links cluster to cloud provider's API  |

### Q5: What is a Deployment in Kubernetes?

**Answer:**
A **Deployment** is a higher-level abstraction that manages the desired state of your Pods and ReplicaSets . It provides declarative updates for applications and is the most common way to deploy stateless applications .

**Key features**:
- **Declarative updates**: Define the desired state, and Kubernetes makes it happen 
- **Rolling updates**: Gradually update Pods without downtime 
- **Rollbacks**: Easily revert to a previous version 
- **Self-healing**: Automatically replaces failed Pods 

**Example Deployment YAML**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
```

### Q6: What is a Service in Kubernetes and what types exist?

**Answer:**
A **Service** is an abstraction that defines a logical set of Pods and a policy for accessing them . Since Pods are ephemeral and their IPs can change, Services provide a stable network endpoint (IP and DNS name) .

**Service types** :

| Type | Description | Access Scope |
|------|-------------|--------------|
| **ClusterIP** (default) | Exposes Service on a cluster-internal IP | Only within cluster |
| **NodePort** | Exposes Service on a static port on each Node's IP | External (NodeIP:NodePort) |
| **LoadBalancer** | Provisions external load balancer from cloud provider | External via cloud LB |
| **ExternalName** | Maps Service to external hostname (CNAME record) | External DNS |

**Example Service YAML**:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
```

### Q7: What are ConfigMaps and Secrets?

**Answer:**
Both **ConfigMaps** and **Secrets** are Kubernetes objects used to inject configuration data into Pods .

| Aspect | ConfigMap | Secret |
|--------|----------|--------|
| **Purpose** | Non-sensitive configuration data | Sensitive data (passwords, API keys, certificates) |
| **Storage** | Plain text | Base64 encoded (can be encrypted at rest) |
| **Use cases** | Environment variables, config files | Credentials, tokens, SSH keys |

**ConfigMap example** :
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  database_url: "postgres://db.example.com"
  log_level: "debug"
```

**Secret example** :
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  password: cGFzc3dvcmQ=  # "password" in base64
```

### Q8: What are Namespaces in Kubernetes?

**Answer:**
**Namespaces** are virtual clusters within a physical Kubernetes cluster . They provide a way to divide cluster resources between multiple users, teams, or projects .

**Common namespaces** :
- **default**: The default namespace for objects with no other namespace
- **kube-system**: Namespace for objects created by the Kubernetes system
- **kube-public**: Readable by all users (reserved for cluster usage)
- **kube-node-lease**: For node heartbeat leases

**Use cases**:
- Multi-tenancy: Isolate environments (dev, staging, prod) 
- Resource quotas: Set limits per namespace
- Access control: RBAC policies scoped to namespaces

### Q9: What are Labels and Selectors?

**Answer:**
**Labels** are key/value pairs attached to Kubernetes objects (like Pods) for identification and organization . **Selectors** are used to filter objects based on these labels .

**Example with labels**:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  labels:
    environment: production
    app: nginx
    tier: frontend
```

**Using selectors**:
```bash
# Select Pods with environment=production
kubectl get pods -l environment=production

# Multiple conditions
kubectl get pods -l 'environment=production,app=nginx'
```

Services and Deployments use label selectors to identify which Pods they should manage .

### Q10: What is kubectl?

**Answer:**
**kubectl** is the command-line tool for interacting with Kubernetes clusters . It communicates with the Kubernetes API server to manage and inspect cluster resources .

**Common commands** :

| Command | Purpose |
|---------|---------|
| `kubectl get pods` | List all Pods in current namespace |
| `kubectl get services` | List all Services |
| `kubectl describe pod <name>` | Show detailed information about a Pod |
| `kubectl logs <pod-name>` | View logs from a Pod |
| `kubectl exec -it <pod-name> -- /bin/sh` | Open interactive shell in a container |
| `kubectl apply -f <file.yaml>` | Create/update resources from YAML |
| `kubectl delete -f <file.yaml>` | Delete resources defined in YAML |

---

## Intermediate Kubernetes Questions

### Q11: What is the difference between a Deployment and a StatefulSet?

**Answer:**
Both manage sets of Pods, but they serve different purposes :

| Aspect | Deployment | StatefulSet |
|--------|------------|-------------|
| **Use case** | Stateless applications | Stateful applications (databases, etc.) |
| **Pod identity** | Pods are interchangeable | Each Pod has stable, unique identity |
| **Storage** | Pods share same PVC or use ephemeral storage | Each Pod gets its own Persistent Volume |
| **Ordering** | No ordering guarantees | Ordered, graceful deployment and scaling |
| **Network identity** | Random pod names | Predictable names (pod-0, pod-1) |

**When to use StatefulSet** :
- Databases (MySQL, PostgreSQL, Cassandra)
- Distributed systems (Kafka, ZooKeeper, Elasticsearch)
- Any application requiring stable network identifiers and persistent storage

### Q12: What are DaemonSets and when would you use them?

**Answer:**
A **DaemonSet** ensures that all (or some) Nodes run a copy of a specific Pod . As nodes are added to the cluster, Pods are automatically added; as nodes are removed, those Pods are garbage collected .

**Common use cases** :
- **Log collection**: Run Fluentd or Filebeat on every node
- **Monitoring**: Deploy Node Exporter or Prometheus node agents
- **Storage**: Run glusterd, ceph, or other storage daemons
- **Networking**: Deploy CNI plugins or kube-proxy alternatives

**Example DaemonSet use**: "I would use a DaemonSet for a log collector like Fluentd that needs to run on every node to gather container logs and forward them to a central logging system."

### Q13: Explain the difference between ReplicaSet and Deployment

**Answer:**
A **ReplicaSet** ensures that a specified number of Pod replicas are running at any given time . However, ReplicaSets are typically not used directlyâ€”**Deployments** manage ReplicaSets and provide additional functionality .

| Feature | ReplicaSet | Deployment |
|---------|------------|------------|
| **Pod count** | Maintains desired number of replicas | Same capability |
| **Rolling updates** | No built-in support | Yes, with controlled rollout |
| **Rollbacks** | No | Yes, easy rollback to previous versions |
| **Pause/resume** | No | Yes, can pause during updates |
| **Typical usage** | Rarely used alone | Standard for stateless apps |

**Relationship**: A Deployment creates and manages a ReplicaSet, which then creates and manages the Pods .

### Q14: What are Persistent Volumes (PVs) and Persistent Volume Claims (PVCs)?

**Answer:**
**Persistent Volumes (PVs)** are cluster resources that provide storage independent of Pod lifecycles . They can be provisioned by administrators or dynamically via StorageClasses .

**Persistent Volume Claims (PVCs)** are requests for storage by users . They specify size, access modes, and storage class.

**Example PV** :
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: "/mnt/data"
```

**Example PVC** :
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```

### Q15: How does Horizontal Pod Autoscaling (HPA) work?

**Answer:**
**Horizontal Pod Autoscaler (HPA)** automatically scales the number of Pod replicas based on observed metrics like CPU, memory, or custom metrics .

**How it works** :
1. HPA periodically queries the Kubernetes Metrics API (typically every 15-30 seconds)
2. It compares current resource usage against target thresholds
3. It calculates desired replicas: `desiredReplicas = ceil(currentReplicas * (currentMetricValue / desiredMetricValue))`
4. It updates the target resource (Deployment/ReplicaSet) with new replica count

**Example HPA YAML**:
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

**Benefits**: Ensures applications can handle varying loads efficientlyâ€”preventing under-provisioning during traffic spikes and over-provisioning during low-demand periods .

### Q16: What are Taints and Tolerations?

**Answer:**
**Taints** are applied to nodes to mark them as unsuitable for certain Pods unless those Pods have matching **Tolerations** . This mechanism controls which Pods can be scheduled on which nodes .

**Example use cases**:
- Dedicated nodes for specific workloads (e.g., GPU nodes)
- Nodes with special hardware
- Evicting Pods from problematic nodes

**Taint example**:
```bash
kubectl taint nodes node1 gpu=true:NoSchedule
```

**Toleration example** in Pod spec:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  tolerations:
  - key: "gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  containers:
  - name: cuda-container
    image: nvidia/cuda:12.0
```

**Taint effects**:
- `NoSchedule`: Pods without matching toleration won't be scheduled
- `PreferNoSchedule`: Kubernetes tries to avoid scheduling
- `NoExecute`: Existing Pods without toleration are evicted

### Q17: What is the difference between NodePort, LoadBalancer, and Ingress?

**Answer:**
These are different ways to expose applications to external traffic :

| Method | Description | Use Case |
|--------|-------------|----------|
| **NodePort** | Exposes Service on static port on each Node's IP | Development, simple testing |
| **LoadBalancer** | Provisions external cloud load balancer | Production when using cloud provider |
| **Ingress** | HTTP/HTTPS routing rules to Services | Advanced routing, multiple Services |

**Ingress** is not a Service type but a separate API object that provides :
- Host-based routing (e.g., `app1.example.com` â†’ Service A)
- Path-based routing (e.g., `/api` â†’ Service B, `/web` â†’ Service C)
- TLS termination
- Load balancing across multiple Services

**Example Ingress**:
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 80
```

### Q18: What are Jobs and CronJobs in Kubernetes?

**Answer:**
**Jobs** are Kubernetes objects designed for batch processing tasks that run to completion . Unlike Deployments (which aim to keep Pods running), Jobs ensure a specified number of Pods successfully terminate .

**CronJobs** are Jobs that run on a scheduled basis, similar to cron in Linux .

**Example Job**:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processor
spec:
  completions: 5
  parallelism: 2
  template:
    spec:
      containers:
      - name: processor
        image: myapp/processor:latest
        command: ["python", "process.py"]
      restartPolicy: Never
```

**Example CronJob**:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-backup
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: backup-tool:latest
            command: ["backup.sh"]
          restartPolicy: OnFailure
```

### Q19: What are Init Containers?

**Answer:**
**Init containers** are specialized containers that run before application containers in a Pod . They run to completion sequentially, and the Pod's main containers only start after all init containers have successfully completed .

**Use cases** :
- Waiting for dependent services (e.g., wait for database to be ready)
- Database schema migrations or seeding
- Setting up file permissions or directory structures
- Pre-fetching data or configuration

**Example**:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
  containers:
  - name: myapp-container
    image: myapp:latest
```

### Q20: What is the role of kube-proxy?

**Answer:**
**kube-proxy** is a network component that runs on every node in the cluster . It maintains network rules and enables communication to Services and Pods .

**Key responsibilities** :
- **Service implementation**: Watches API server for Service and Endpoint changes
- **Traffic forwarding**: Forwards traffic destined for Service IPs to actual Pod IPs
- **Load balancing**: Distributes traffic across Pod endpoints for a Service
- **Mode options**: Can use iptables, IPVS, or userspace mode

**How it works**:
1. kube-proxy monitors the API server for Service and Endpoint changes
2. It updates iptables/IPVS rules on the node
3. When traffic hits a Service IP, these rules redirect it to healthy backend Pods

---

## Advanced Kubernetes Questions

### Q21: Explain Kubernetes Network Policies and how they work

**Answer:**
**Network Policies** are specifications that define how groups of Pods are allowed to communicate with each other and with external endpoints . They act as a firewall for Pods, implementing network segmentation .

**How they work**:
- Network Policies are implemented by a CNI plugin that supports them (Calico, Cilium, Weave Net) 
- The plugin translates policies into firewall rules (iptables, eBPF) 
- By default, Pods accept traffic from any source (no isolation)
- Once a Pod is selected by any NetworkPolicy, it becomes isolated and only accepts traffic allowed by its policies

**Example NetworkPolicy** (allow traffic from frontend Pods to backend on port 3306):
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 3306
```

### Q22: What are Operators and Custom Resource Definitions (CRDs)?

**Answer:**
**Custom Resource Definitions (CRDs)** allow you to extend the Kubernetes API by defining your own custom resource types . They enable Kubernetes to store and retrieve structured data that the platform doesn't natively understand .

**Operators** are software extensions that package, deploy, and manage complex stateful applications using custom controllers and CRDs . They encode human operational knowledge into software .

**Example use cases**:
- Database operators (Cassandra, MySQL, PostgreSQL)
- Message queue operators (Kafka, RabbitMQ)
- Backup operators (Velero)

**How Operators work**:
1. Define a CRD for your application (e.g., `PostgresCluster`)
2. Write a controller that watches for changes to CRD instances
3. Controller reconciles actual state to desired state by creating/updating Kubernetes resources
4. Operator handles backups, upgrades, scaling, and failure recovery automatically

### Q23: How does the Kubernetes API server handle authentication and authorization?

**Answer:**
The API server processes requests through multiple stages :

**Authentication** (who you are):
- **Client certificates**: TLS mutual authentication
- **Bearer tokens**: Service account tokens, OIDC tokens
- **HTTP basic auth**: Username/password (deprecated)
- **Authenticating proxy**: Headers from proxy
- **Anonymous requests**: Can be enabled/disabled

**Authorization** (what you can do):
- **RBAC (Role-Based Access Control)**: Most common, defines roles with permissions and bindings 
- **ABAC (Attribute-Based Access Control)**: Policies based on user attributes (complex, rarely used)
- **Node authorization**: Special authorizer for kubelet
- **Webhook**: External authorization service

**Admission Control** (after authN/authZ, before persistence):
- Mutating admission webhooks: Can modify requests 
- Validating admission webhooks: Validate requests
- Built-in admission controllers: ResourceQuota, PodSecurity, etc.

**Example RBAC**:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

### Q24: How does etcd work and why is it critical?

**Answer:**
**etcd** is a distributed, consistent, and highly-available key-value store that serves as Kubernetes' backing store for all cluster data . It's the single source of truth for the cluster .

**Key features** :
- **Raft consensus algorithm**: Ensures data consistency across nodes
- **High availability**: Can tolerate node failures (with proper quorum)
- **Watch support**: Clients can watch for changes
- **TTL**: Keys can expire (used for lease-based locks)

**What etcd stores**:
- Cluster configuration
- State information for all objects (Pods, Deployments, Services)
- Metadata and annotations
- Secrets (encrypted if configured)

**Critical considerations**:
- etcd availability directly impacts cluster healthâ€”if etcd goes down, the cluster cannot change state
- For production, etcd should be deployed as a cluster (typically 3, 5, or 7 nodes)
- Regular backups are essential for disaster recovery
- etcd performance affects overall cluster responsiveness

### Q25: Explain the concept of Pod Disruption Budgets (PDBs)

**Answer:**
**Pod Disruption Budgets (PDBs)** limit the number of Pods from a collection that can be down simultaneously during voluntary disruptions . They help maintain application availability during maintenance operations.

**Voluntary disruptions** include:
- Node drains (for maintenance or upgrades)
- Cluster upgrades
- Deletion of Deployments/StatefulSets
- Manual Pod deletion

**Involuntary disruptions** (not covered by PDBs):
- Hardware failures
- Network partitions
- Out-of-memory kills

**PDB examples**:
```yaml
# At least 2 pods must always be available
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp
```

```yaml
# At most 1 pod can be unavailable (works with 3+ replicas)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: myapp
```

**How they work**: When performing operations like `kubectl drain`, Kubernetes checks PDBs and delays the operation if it would violate the budget.

### Q26: What are the different container runtimes supported by Kubernetes?

**Answer:**
Kubernetes supports any container runtime that implements the **Container Runtime Interface (CRI)** . CRI is a plugin interface that enables kubelet to use different runtimes without recompiling.

**Supported runtimes** :

| Runtime | Description | Notes |
|---------|-------------|-------|
| **containerd** | Industry-standard, CNCF-graduated runtime | Simple, stable, production-ready |
| **CRI-O** | Lightweight, OCI-compliant runtime | Built specifically for Kubernetes |
| **Docker Engine** | Popular, feature-rich runtime | Deprecated in v1.24+, still works via cri-dockerd |
| **Mirantis Container Runtime** | Formerly Docker Enterprise | Enterprise-focused |

**Historical note**: Docker support was removed in Kubernetes 1.24 because Docker didn't implement CRI. However, you can still use Docker via the **cri-dockerd** adapter .

### Q27: What is the role of the Scheduler and how does it make decisions?

**Answer:**
The **kube-scheduler** is responsible for assigning newly created Pods to nodes that can run them . It watches the API server for unscheduled Pods and selects appropriate nodes .

**Scheduling process** :

1. **Filtering (Predicates)**: Eliminates nodes that can't run the Pod
   - Resource requirements (CPU/memory requests)
   - NodeSelector and NodeAffinity
   - PodFitsHostPorts
   - PodToleratesNodeTaints
   - PodFitsResources

2. **Scoring (Priorities)**: Ranks remaining nodes
   - LeastRequestedPriority: Prefer nodes with more available resources
   - BalancedResourceAllocation: Prefer balanced resource usage
   - ImageLocality: Prefer nodes with required container images
   - NodeAffinityPriority

3. **Binding**: Selects highest-scoring node and binds Pod

**Custom schedulers**: You can write your own scheduler or run multiple schedulers in a cluster for specialized workloads.

### Q28: How does Kubernetes handle DNS and service discovery?

**Answer:**
Kubernetes provides built-in DNS-based service discovery through **CoreDNS** (or previously kube-dns) .

**How it works** :
- CoreDNS runs as a Deployment in the cluster
- kubelet configures each Pod's `/etc/resolv.conf` to use CoreDNS
- Services get DNS records in the format: `<service-name>.<namespace>.svc.cluster.local`
- Pods get DNS records (optional): `<pod-ip>.<namespace>.pod.cluster.local`

**DNS naming conventions**:
- **Headless Services** (clusterIP: None): DNS returns multiple A records (all Pod IPs)
- **Normal Services**: DNS returns ClusterIP
- **ExternalName Services**: DNS returns CNAME record to external name

**Service discovery example**: A Pod in the `default` namespace can access `database-service` in the `prod` namespace at `database-service.prod.svc.cluster.local`.

### Q29: Explain Kubernetes Security Context

**Answer:**
**Security Context** defines privilege and access control settings for a Pod or container . It's how you implement the principle of least privilege in Kubernetes.

**Security Context settings** :

| Setting | Description |
|---------|-------------|
| `runAsUser` | User ID to run container processes |
| `runAsGroup` | Group ID to run container processes |
| `fsGroup` | Group ID for volume ownership |
| `runAsNonRoot` | Prevent running as root (true/false) |
| `privileged` | Run in privileged mode (avoid if possible) |
| `capabilities` | Linux capabilities to add/drop (e.g., `NET_BIND_SERVICE`) |
| `readOnlyRootFilesystem` | Mount root filesystem as read-only |
| `allowPrivilegeEscalation` | Control whether processes can gain more privileges |

**Example Pod-level security context**:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
  containers:
  - name: secure-container
    image: myapp:latest
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop: ["ALL"]
        add: ["NET_BIND_SERVICE"]
```

### Q30: What are Pod Topology Spread Constraints?

**Answer:**
**Pod Topology Spread Constraints** control how Pods are distributed across your cluster's failure domains (zones, nodes, etc.) . They help achieve high availability and efficient resource utilization.

**Example use cases**:
- Spread Pods across availability zones for HA
- Distribute Pods evenly across nodes to avoid hotspots
- Ensure critical workloads are not co-located

**Example constraint**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spread-demo
spec:
  replicas: 9
  selector:
    matchLabels:
      app: spread-demo
  template:
    metadata:
      labels:
        app: spread-demo
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: spread-demo
      containers:
      - name: nginx
        image: nginx
```

**Key concepts**:
- **topologyKey**: The node label representing the domain (zone, region, hostname)
- **maxSkew**: Maximum difference in Pod count across domains
- **whenUnsatisfiable**: `DoNotSchedule` (block) or `ScheduleAnyway` (best effort)
- **labelSelector**: Which Pods to consider for distribution

---

## Kubernetes Scenario-Based Questions

### Q31: Pods are frequently restarting. How would you troubleshoot?

**Scenario:** Your application Pods are in a `CrashLoopBackOff` state or have a high restart count .

**Answer:**
Systematic troubleshooting approach :

**Step 1: Check Pod status and events**
```bash
# Get pod status and restart count
kubectl get pods

# View detailed events
kubectl describe pod <pod-name>
```
Events often reveal issues like failed probes, OOM kills, or image pull errors .

**Step 2: Check container logs**
```bash
# View current container logs
kubectl logs <pod-name>

# View previous container logs (before crash)
kubectl logs <pod-name> --previous
```

**Step 3: Check resource constraints**
```bash
# Check if Pod hit memory limit
kubectl describe pod <pod-name> | grep -A 5 "State:"
```
Look for `OOMKilled` in the state .

**Step 4: Common causes** :
- **Image issues**: Wrong image path, missing tag
- **Application bugs**: Application crashing on startup
- **Resource limits**: Insufficient memory leading to OOM kills
- **Liveness probe failures**: Probe failing, causing restart
- **Configuration errors**: Missing ConfigMaps or Secrets

**Step 5: Check liveness/readiness probes**:
```bash
kubectl get pod <pod-name> -o yaml | grep -A 10 "livenessProbe"
```

### Q32: Service cannot connect to Pods

**Scenario:** You created a Service, but it's not routing traffic to your Pods .

**Answer:**
Follow this debugging approach :

**Step 1: Verify Pods are running and healthy**
```bash
kubectl get pods -l app=your-app
```
Ensure Pods show `Running` status.

**Step 2: Check Service endpoints**
```bash
kubectl get endpoints <service-name>
```
If endpoints are empty, the Service selector isn't matching any Pods .

**Step 3: Verify label matching**
```bash
# Check Service selector
kubectl describe service <service-name> | grep Selector

# Check Pod labels
kubectl get pods --show-labels
```
Ensure the labels match exactly.

**Step 4: Check Service type and access method**
- For `ClusterIP`: Test from within cluster (`kubectl run test --rm -it --image=busybox -- wget -O- http://service-name:port`)
- For `NodePort`: Check firewall rules and node accessibility

**Step 5: Check Network Policies** :
```bash
kubectl get networkpolicies
```
Network Policies might be blocking traffic .

**Step 6: Check kube-proxy and DNS**
```bash
# Check if service IP exists
kubectl get svc

# Test DNS resolution from a Pod
kubectl exec -it <pod-name> -- nslookup <service-name>
```

### Q33: Node is in NotReady state

**Scenario:** One of your worker nodes shows `NotReady` status. How do you diagnose and fix it?

**Answer:**
Troubleshooting steps for a NotReady node:

**Step 1: Check node status**
```bash
kubectl get nodes
kubectl describe node <node-name>
```
Look for conditions and events.

**Step 2: Check node resources**
```bash
# From the node itself
df -h  # Check disk space
free -m  # Check memory
top  # Check CPU/load
```
Disk pressure, memory pressure, or PID pressure can cause NotReady status.

**Step 3: Check kubelet status on the node**
```bash
# SSH to the node
sudo systemctl status kubelet
sudo journalctl -u kubelet -f
```

**Step 4: Check container runtime**
```bash
sudo systemctl status containerd  # or docker
```

**Step 5: Common causes**:
- **Resource exhaustion**: Node out of disk, memory, or PIDs
- **Kubelet issues**: Kubelet stopped or crashed
- **Network issues**: Node cannot reach API server
- **Container runtime problems**: containerd/docker not responding

**Step 6: Recovery actions**:
- Free up disk space
- Restart kubelet: `sudo systemctl restart kubelet`
- Restart container runtime
- If all else fails, cordon and drain then reboot

### Q34: Rolling update failed. How do you rollback?

**Scenario:** You deployed a new version of your application, but it's failing. You need to roll back quickly .

**Answer:**
Kubernetes provides built-in rollback capabilities :

**Step 1: Check rollout status**
```bash
kubectl rollout status deployment/<deployment-name>
```

**Step 2: View rollout history**
```bash
kubectl rollout history deployment/<deployment-name>
```

**Step 3: Roll back to previous version**
```bash
# Rollback to previous revision
kubectl rollout undo deployment/<deployment-name>

# Rollback to specific revision
kubectl rollout undo deployment/<deployment-name> --to-revision=2
```

**Step 4: Verify rollback**
```bash
kubectl rollout status deployment/<deployment-name>
kubectl get pods
```

**Alternative approach**: "Rolling forward" with fixed manifests :
- Fix the issue in your YAML files
- Reapply with `kubectl apply -f fixed-deployment.yaml`
- This is often preferred in GitOps workflows 

### Q35: Application running out of memory

**Scenario:** Your application keeps getting OOMKilled. How do you diagnose and fix it?

**Answer:**
Memory-related issues require systematic investigation:

**Step 1: Verify OOM kills**
```bash
kubectl describe pod <pod-name> | grep -A 10 "State:"
```
Look for `OOMKilled` in the container state.

**Step 2: Check current memory usage**
```bash
# Install metrics server first if not present
kubectl top pods
kubectl top nodes
```

**Step 3: Review resource requests and limits**
```bash
kubectl get pod <pod-name> -o yaml | grep -A 5 resources
```
Check if limits are set too low.

**Step 4: Analyze application memory patterns**
- Check application logs for memory leaks
- Monitor memory usage over time
- Use profiling tools if possible

**Step 5: Solutions**:

**Option A: Increase memory limits**:
```yaml
resources:
  requests:
    memory: "512Mi"
  limits:
    memory: "1Gi"
```

**Option B: Implement Horizontal Pod Autoscaling** to spread load

**Option C: Optimize application memory usage**
- Fix memory leaks
- Adjust garbage collection
- Cache optimization

### Q36: Exposing application securely to the internet

**Scenario:** You need to expose a web application running in Kubernetes to the internet securely .

**Answer:**
Secure exposure typically involves multiple layers :

**Recommended approach: Ingress + TLS + authentication**

**Step 1: Deploy an Ingress Controller** (if not present)
```bash
# Example: Nginx Ingress
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml
```

**Step 2: Create a Service** for your application
```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080
```

**Step 3: Create Ingress with TLS** 
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - myapp.example.com
    secretName: myapp-tls
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp-service
            port:
              number: 80
```

**Step 4: Add authentication** (optional)
- Use OAuth2 proxy for authentication
- Implement JWT validation at Ingress level
- Use service mesh (Istio/Linkerd) for mTLS

**Alternative: LoadBalancer Service** :
```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-lb
spec:
  type: LoadBalancer
  selector:
    app: myapp
  ports:
  - port: 443
    targetPort: 8443
```

### Q37: Cluster running out of resources

**Scenario:** Your cluster nodes are running low on CPU or memory, affecting application performance .

**Answer:**
Multi-step approach to diagnose and mitigate :

**Step 1: Identify resource hogs**
```bash
# Check node resource usage
kubectl top nodes

# Check pod resource usage
kubectl top pods --all-namespaces | sort -k3 -n
```

**Step 2: Check resource quotas and limits**
```bash
# Check if pods have appropriate limits
kubectl get pods --all-namespaces -o json | jq '.items[] | {name: .metadata.name, namespace: .metadata.namespace, resources: .spec.containers[].resources}'
```

**Step 3: Identify waste**
- Look for pods with no resource limits (can consume all node resources)
- Check for pods stuck in `Pending` or `CrashLoopBackOff`
- Identify large pods that could be right-sized

**Step 4: Mitigation strategies** :

**Immediate actions:**
- Scale cluster horizontally (add more nodes)
- Evict non-critical pods
- Reschedule batch jobs to off-peak hours

**Long-term solutions:**
- Implement Horizontal Pod Autoscaling 
- Set appropriate resource requests and limits for all workloads
- Use cluster autoscaling (if supported by cloud provider)
- Implement priority classes for critical workloads
- Use taints and tolerations to isolate workloads 

**Example resource quotas**:
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
```

### Q38: Stateful application data persistence

**Scenario:** You're deploying a database in Kubernetes and need to ensure data isn't lost when Pods restart or reschedule .

**Answer:**
Use StatefulSet with PersistentVolumes :

**Step 1: Create a StorageClass** (for dynamic provisioning)
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs  # cloud-specific
parameters:
  type: gp3
```

**Step 2: Create a StatefulSet with volumeClaimTemplates** 
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "fast-ssd"
      resources:
        requests:
          storage: 100Gi
```

**Key points** :
- StatefulSet provides stable network identities (pod-0, pod-1, etc.)
- Each pod gets its own PersistentVolume via volumeClaimTemplates
- Volumes persist even if pods are rescheduled
- For production, use StatefulSets with regular backups
- Consider database operators for production (e.g., Zalando Postgres Operator) 

### Q39: Multi-tenancy in a shared cluster

**Scenario:** Multiple teams need to share a Kubernetes cluster. How do you ensure isolation and resource fairness?

**Answer:**
Implement multi-tenancy using several Kubernetes features:

**1. Namespaces for logical isolation** :
```bash
kubectl create namespace team-a
kubectl create namespace team-b
```

**2. Resource Quotas per namespace** :
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-quota
  namespace: team-a
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    persistentvolumeclaims: "5"
    pods: "20"
    services: "10"
```

**3. LimitRanges for default resource constraints**:
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: team-limits
  namespace: team-a
spec:
  limits:
  - max:
      cpu: "2"
      memory: 4Gi
    min:
      cpu: "100m"
      memory: 256Mi
    default:
      cpu: "500m"
      memory: 1Gi
    defaultRequest:
      cpu: "200m"
      memory: 512Mi
    type: Container
```

**4. RBAC for access control** :
```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: team-a
  name: team-member
rules:
- apiGroups: ["", "apps"]
  resources: ["pods", "deployments", "services"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: team-a-binding
  namespace: team-a
subjects:
- kind: Group
  name: team-a@company.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: team-member
  apiGroup: rbac.authorization.k8s.io
```

**5. Network Policies for network isolation** :
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-cross-namespace
  namespace: team-a
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: team-a
```

### Q40: Zero-downtime deployment strategy

**Scenario:** You need to update a critical application with zero downtime .

**Answer:**
Implement a rolling update strategy with proper health checks :

**1. Configure Deployment with rolling update strategy**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Can create 1 extra pod
      maxUnavailable: 0  # Must keep all pods available during update
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myapp:2.0.0
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 20
```

**2. Implement PodDisruptionBudget** :
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 4
  selector:
    matchLabels:
      app: myapp
```

**3. Use Service with proper selector**:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080
```

**4. Perform the update**:
```bash
# Update image
kubectl set image deployment/myapp myapp=myapp:2.0.0

# Monitor rollout
kubectl rollout status deployment/myapp
```

**Key principles** :
- **Readiness probes**: Ensure new pods receive traffic only when ready
- **Rolling update strategy**: Controls pace of pod replacement
- **PDB**: Ensures minimum availability during voluntary disruptions
- **Canary deployments**: Route small percentage of traffic to new version first

---

## Kubernetes Commands Cheat Sheet

| Category | Command | Description |
|----------|---------|-------------|
| **Cluster Info** | `kubectl cluster-info` | Display cluster information |
| | `kubectl get nodes` | List all nodes in cluster |
| | `kubectl get namespaces` | List all namespaces |
| | `kubectl api-resources` | List all API resources |
| **Workloads** | `kubectl get pods` | List pods in current namespace |
| | `kubectl get pods -A` | List pods across all namespaces |
| | `kubectl get deployments` | List deployments |
| | `kubectl get statefulsets` | List statefulsets |
| | `kubectl get daemonsets` | List daemonsets |
| | `kubectl get jobs` | List jobs |
| | `kubectl get cronjobs` | List cronjobs |
| **Services & Networking** | `kubectl get services` | List services |
| | `kubectl get endpoints` | List endpoints |
| | `kubectl get ingress` | List ingress resources |
| | `kubectl get networkpolicies` | List network policies |
| **Configuration** | `kubectl get configmaps` | List configmaps |
| | `kubectl get secrets` | List secrets |
| | `kubectl get serviceaccounts` | List service accounts |
| **Storage** | `kubectl get pv` | List persistent volumes |
| | `kubectl get pvc` | List persistent volume claims |
| | `kubectl get storageclass` | List storage classes |
| **RBAC** | `kubectl get roles` | List roles |
| | `kubectl get rolebindings` | List role bindings |
| | `kubectl get clusterroles` | List cluster roles |
| | `kubectl get clusterrolebindings` | List cluster role bindings |
| **Debugging** | `kubectl describe pod <pod>` | Show detailed pod info |
| | `kubectl logs <pod>` | View pod logs |
| | `kubectl logs -f <pod>` | Stream pod logs |
| | `kubectl logs <pod> -c <container>` | View specific container logs |
| | `kubectl exec -it <pod> -- /bin/sh` | Execute command in pod |
| | `kubectl port-forward <pod> 8080:80` | Forward local port to pod |
| | `kubectl top pod` | Show pod resource usage |
| | `kubectl top node` | Show node resource usage |
| **Resource Management** | `kubectl apply -f <file.yaml>` | Create/update from file |
| | `kubectl delete -f <file.yaml>` | Delete resources from file |
| | `kubectl delete pod <pod>` | Delete specific pod |
| | `kubectl edit deployment <name>` | Edit deployment in editor |
| | `kubectl scale deployment <name> --replicas=5` | Scale deployment |
| | `kubectl rollout status deployment/<name>` | Check rollout status |
| | `kubectl rollout history deployment/<name>` | View rollout history |
| | `kubectl rollout undo deployment/<name>` | Rollback deployment |
| | `kubectl set image deployment/<name> container=image:tag` | Update image |
| **Label & Annotation** | `kubectl label pod <pod> key=value` | Add/update label |
| | `kubectl annotate pod <pod> key=value` | Add/update annotation |
| | `kubectl get pods -l key=value` | Filter by label |
| **Namespace Operations** | `kubectl create namespace <name>` | Create namespace |
| | `kubectl delete namespace <name>` | Delete namespace |
| | `kubectl config set-context --current --namespace=<name>` | Set default namespace |
| **Node Management** | `kubectl cordon <node>` | Mark node unschedulable |
| | `kubectl uncordon <node>` | Mark node schedulable |
| | `kubectl drain <node>` | Drain node (evict pods) |
| | `kubectl taint nodes <node> key=value:effect` | Add taint |
| **Context & Config** | `kubectl config get-contexts` | List contexts |
| | `kubectl config use-context <context>` | Switch context |
| | `kubectl config current-context` | Show current context |
| | `kubectl config view` | View kubeconfig |

---

## ðŸ’¡ Tips for Kubernetes Interview Success

1. **Understand fundamentals deeply** - Be crystal clear on Pods, Services, Deployments, and the control plane components 

2. **Master troubleshooting** - Practice debugging scenarios: crash loops, service connectivity, node issues 

3. **Know the ecosystem** - Understand CNI plugins, Ingress controllers, service mesh, and monitoring tools 

4. **Hands-on practice** - Set up local clusters with Minikube or kind, deploy real applications, break things and fix them 

5. **Security mindset** - Be prepared to discuss RBAC, network policies, PodSecurity standards, and secrets management 

6. **Networking knowledge** - Understand kube-proxy, CNI, service types, DNS, and network policies 

7. **Stateful applications** - Know StatefulSets, persistent volumes, and operators for stateful workloads 

8. **Scheduling concepts** - Understand taints/tolerations, node affinity, pod topology spread constraints 

9. **STAR method for scenarios** - Structure scenario answers with Situation, Task, Action, Result 

10. **Stay current** - Keep up with recent Kubernetes releases, deprecations (like Docker Shim removal), and new features 

Good luck with your Kubernetes interview! ðŸš€